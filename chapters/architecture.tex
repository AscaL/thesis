%TODO: maybe explain other tecniques
In this chapter we will give a brief introduction of Machine Learning (\ref{ml}), explaining Classification (\ref{classification}), Regression (\ref{regression}), Supervised (\ref{supervised}) and Unsupervised (\ref{unsupervised}) Learning, proceding then to explain SVM (\ref{svm}) which is the most important technique used for this thesis.

\section{Machine Learning} \label{ml}
Machine Learning is a subfield of Artificial Intelligence that uses statistical techniques to provide computers with the ability to progressively improve performance on different tasks using data, while not being explicitly programmed \cite{wiki:ml}. \\
The application for machine learning are huge and diverse, and range from character recognition to email filtering, with lots of application in computer vision, such as image recognition and classification. \\
Machine learning also focuses on making prediction on data, by utilizing techniques taken from mathematical optimization. This has many applications, ranging from health care by predicting risk factors for diseases or gaining insights for prevention, to sports or politics where actual results can be predicted.\\
The field of machine learning is subdivided in two broad categories (Fig. \ref{fig:ml_mldiv}), supervised learning (\ref{supervised}) and unsupervised learning (Fig. \ref{unsupervised}) based on whether the data is labeled or not.\\

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{ml_mldiv}
	\caption{Machine Learning Subfields \cite{ml_mldiv}}
	\label{fig:ml_mldiv}
\end{figure}

Another classification is based on the kind of output that the users wish to obtain, mainly Classification (\ref{classification}), Regression (\ref{regression}) or Clustering.

% TODO: Section or SubSection?
\subsection{Supervised Learning} \label{supervised}
When using supervised learning we want to learn a function that maps an input to an output, based on example input-output pairs \cite{ai_sup}. This means that we need labeled training data, consisting of a vector of input objects and an output value. The objective is to correctly classify the new data based on the analyzed training pairs. \\
The general steps to solve a supervised learning problem are (Fig. \ref{fig:sup_wf}):
\begin{enumerate}[noitemsep]
	\item Understand what kind of training example to use.
	\item Gathering a training set.
	\item Model the training set to be fed as input to the algorithm by choosing which features to use and how to represent the data.
	\item Choose what kind of algorithm can best train the model.
	\item Run the algorithm and evaluate the resulting accuracy on the test set 
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{sup_wf}
	\caption{Supervised Learning Workflow \cite{sup_wf}}
	\label{fig:sup_wf}
\end{figure}

There are important considerations to make when using a supervised learning approach:
\begin{itemize}
	\item \textbf{Dimensionality of Input}: When the input feature vectors are very big there could be problems in learning the function, even if not all features contribute significantly to the function. This happens because the data depends on too many variables and this could cause high variance. \\
	To avoid this, it is important to reduce the number of features through manual removal or using feature selection algorithms. This usually improves the accuracy of the classifier.
	\item \textbf{Overfitting and Underfitting} \cite{overfit}: Overfitting (or overtraining) happens when the algorithm adapts too much on the training data and is no longer able to make accurate predictions on the test data (Fig. \ref{fig:ml_overfit}). This usually happens when there is an excessive number of parameters than can be justified by the data \cite{camb_over}. \\
	Underfitting is the opposite: a model is not able to correctly capture the structure of the data, for example when fitting non linear data with a linear model. \\
	A common way to avoid overfitting is to resample the data using different techniques, commonly k-fold cross validation or leave-one-out. Other methods include feature removal, early stopping, regularization.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{ml_overfit}
		\caption{Example of Overfitting (green line) \cite{wiki:ml_overfit}}
		\label{fig:ml_overfit}
	\end{figure}	
	
	% TODO: Write this better
	\item \textbf{Bias-Variance Tradeoff} \cite{biasvar}: suppose we have different (but equally good) training set. An algorithm is biased for input x if, when trained on each of these data sets, it is consistently incorrect at predicting the correct output for x. \\
	A learning algorithm has high variance for a particular input x if it predicts different output values when trained on different training sets. \\
	The prediction error of a classifier is related to the sum of bias and variance, so generally there is a tradeoff between them. Low bias means that it fits the new data well, but if the bias is too low it will fit each training set differently and so result in high variance.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{biasvar}
		\caption{Example of Bias-Variance Tradeoff \cite{biasvarTradeoff}}
		\label{fig:biasvar}
	\end{figure}	
\end{itemize}

There are many algorithms used to perform supervised learning tasks, the most commonly used are: \\
\begin{itemize}[noitemsep, topsep = -5pt]
	\item Linear Regression
	\item Logistic Regression
	\item Naive Bayes
	\item Linear Discriminant Analysis
	\item Decision Trees
	\item k-Nearest Neighbor
	\item Neural Networks
	\item Support Vector Machines (\ref{svm})
\end{itemize}

% TODO: Should i write how sup learning works? (from wiki section)

\subsection{Unsupervised Learning} \label{unsupervised}
Unsupervised learning is the subfield of Machine Learning tasked with inferring a function from the analysis of unlabeled data (not classified). Being unclassified there is also a difficulty in to evaluate the accuracy of the model.\\
Usually items are grouped by some measure of similarity, like for example in k-means clustering (Fig. \ref{fig:clustering}).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{clustering}
	\caption{Example of Unclustered and Clustered data \cite{kmeans}}
	\label{fig:clustering}
\end{figure}	

These are the most widely used unsupervised learning algorithms:

\begin{itemize}[noitemsep]
	\item Clustering
	\begin{itemize}[noitemsep]
		\item k-means
		\item mixture models
		\item hierarchical clustering
	\end{itemize}
	\item Anomaly detection
	\item Neural Networks
	\begin{itemize}[noitemsep]
		\item Autoencoders
		\item Deep Belief Nets
		\item Hebbian Learning
		\item Generative Adversarial Networks
		\item Self-organizing map
	\end{itemize}
	\item Expectationâ€“maximization algorithm (EM)
	\item Method of moments
	\item Blind signal separation techniques
	\begin{itemize}[noitemsep]
		\item Principal component analysis,
		\item Independent component analysis,
		\item Non-negative matrix factorization
		\item Singular value decomposition.
	\end{itemize}
\end{itemize}

\subsection{Classification} \label{classification}
In machine learning, classification is the problem of identifying in which of a set of categories a new observation belongs, based on a training set of data containing observations whose category is known in advance. A common example is classifying an email as spam or not.\\
Classification is considered an instance of supervised learning, based on instances where a training set is available. As for unsupervised learning, classification would be clustering since it groups data into categories based on similarity, but without knowing the label of the data.\\
The observations, called features or explanatory variables, take different types based on the value. They can be categorical, numerical or ordinal, or be compared by similarity between previous observations using some kind of distance function.\\
The observations representing the categories to be predicted are called explanatory variables (or regressors, or independent variables).\\
The classifier is the algorithm that implements the classification, or a function that maps input data to a category.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{classification}
	\caption{Example of Classification \cite{}}
	\label{fig:classification}
\end{figure}	

Many classification algorithm, such as SVM (\ref{svm}), logistic regression, LDA (Linear Discriminant Analysis) or perceptron, can be described using a linear function assigning a score to each category c, by doing the dot product of the feature vector of an instance with a vector of weights, thus combining them. The predicted category will be the one with the highest score. This function is called linear predictor function and has this general formula:

\begin{equation}
score(X_i, c) = \beta_c \cdot X_i
\end{equation}

where $X_i$ is the feature vector of instance i and $\beta_c$ is the vector of weights of category c. This kind of algorithms are known as linear classifiers.

\subsection{Regression Analysis} \label{regression}
%TODO: riscrivere e' messo di merda
Regression analysis is used in statistics and machine learning to estimate the relationships between variables. It focus on the relationship between one dependent variable and more independent variables (also called predictors) and it is subsequently used to make predictions on how changing some of the  independent variables will affect the dependent one. \cite{wiki:reg_an} \\
Regression is generally used to estimate the average value of the dependent variable when the independent variables are fixed. In doing that a regression function is calculated. Another use of regression analysis is to understand the relationship among the independent and dependent variables.
In regression analysis, it is also of interest to characterize the variation of the dependent variable around the prediction of the regression function using a probability distribution.

%TODO: dove metto ste figure? e' troppo stupida la seconda?
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{reg_plot}
	\caption{Example of Regression Analysis}
	\label{fig:reg_plot}
\end{figure}	

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{reg_analysis}
	\caption{Regression Analysis can solve this! \cite{reg_analisys}}
	\label{fig:reg_analysis}
\end{figure}

\textbf{Regression Model} \\
A general regression model uses the following variables and parameters:
The unknown parameters, $\beta$, that represents either a scalar or a vector.\\
The independent variables, $X$. \\
The dependent variable, $Y$.\\

A regression model relates $Y$ to a function of $X$ and $\beta$.
\begin{equation}
	Y \approx f(X,\beta)
\end{equation}

This is usually formalized as
\begin{equation}
	E(Y|X) = f(X,\beta)
\end{equation}

%TODO: riguardare anche questo, troppo wiki
If $\beta$ is of length $k$ and the number of observed data points is enough ($N > k$), then it's possible to estimate a unique value for $\beta$ that best fits the data. If this is the case, regression analysys provides the means to find a solution for unknown parameters of $\beta$ to, for example, apply the method of least squares.

To apply regression the data must abide by some assumption, generally:
\begin{itemize}[noitemsep, topsep = -5pt]
	\item The sample is representative of the population for the inference prediction.
	\item The error is a random variable with a mean of zero.
	\item The independent variables are measured with no error.
	\item The independent variables (predictors) are linearly independent.
	\item The errors are uncorrelated.
	\item The variance of the error is constant across observations.
\end{itemize}

%TODO: section or sub?
\subsection{Linear Regression}
Linear regression is the most basic case of Regression Analysis, and it is a useful tool for predicting a quantitative response. Also most of the newer approaches to regression are often a generalization or extension of linear regression. \\
Linear regression is a linear approach to modeling the relationship between a dependent variable and one or more independent variables. The simplest case of linear regression (LR) is when there is only one independent variable, and is called simple linear regression \cite{wiki:lin_reg} and has this form:
\begin{equation}
	y_i \approx \beta_0 + \beta_1x_i + \epsilon_i
\end{equation}
where $\epsilon_i$ is the error for the i-th observation.\\
The point of LR is to estimate the $\beta$ coefficients to make predictions. To do so we make use of the training data to get the estimates $\widehat{\beta_0}$ and $\widehat{\beta_1}$ for the model coefficients. We can then make predictions by computing:
\begin{equation}
\widehat{y_i} \approx \widehat{\beta_0} + \widehat{\beta_1x_i}
\end{equation}

$e_i = y_i - \widehat{y_i}$ is the difference between the true value and the prediction  for an observation and it is called residual. 
The most common method for estimation is called least squares. This method obtains parameter estimates that minimize the sum of squared residuals (SSR):
\begin{equation}
	SSR=\sum _{i=1}^{n}e_{i}^{2}
\end{equation}

The minimizers are
\begin{equation}
	 \widehat{\beta_{1}} = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}
\end{equation}
where $\bar{x}$ and $\bar{y}$ are the mean of $x$ and $y$

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{reg_error}
	\caption{The fit is found by minimizing the sum of squared errors. Each gray line segment represents an error, and the fit makes a compromise by averaging their square}
	\label{fig:reg_error}
\end{figure}




\pagebreak

\section{Random Forest}

\section{SVM} \label{svm}
Support Vector Machines (SVM) are a supervised machine learning algorithm used for both classification and regression. \\
The main idea is to find the optimal hyperplane for linearly separable data, and then extend this idea to data that are not linearly separable by mapping this data in a new space using a kernel function.\\
The definition of an hyperplane for p-dimensions is:
\begin{equation}
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p = 0
\end{equation}

Support vectors are the data points that lie closest to the hyperplane (Fig. \ref{fig:suppvec}), they also are the data points most difficult to classify and have direct bearing on the optimum location of the hyperplane.\\
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{svm_support_vectors}
	\caption{Example of Support Vectors}
	\label{fig:suppvec}
\end{figure}

The distance between the hyperplane and the nearest data point from either set is known as the margin. The best hyperplane is the one that maximizes the margins for the data we are classifying.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{hyperplane_margin}
	\caption{Different hyperplanes with different margins \cite{svm_monkeylearn}}
\end{figure}

So the objective is to choose a hyperplane with the largest possible margin between it and the support vectors, since the larger is the margin, the lower the generalization error of the classifier.\\
In essence, support vectors are the elements of the training set that would change the position of the dividing hyperplane if removed. This makes the support vectors the critical elements of the training set.\\

Finding the maximal margin hyperplane based on a set of training observations $x_1, \dots x_n \in R^p$ and with class lables $y_1 \dots y_n$, translates to an optimization problem:

Maximize M
\begin{equation}
\beta_0, \beta_1, \beta_2, \dots, \beta_p, M
\end{equation}

subject to
\begin{equation}
\sum_{j=1}^{p}\beta^2_j = 1
\end{equation}

\begin{equation}
y_i(\beta_0 + \beta_1 X_i1 + \beta_2 X_i2 + \dots + \beta_p X_ip) \ge M \forall i = 1, \dots, n
\end{equation}

2.3 and 2.4 ensure that each observation is on the correct side of the hyperplane and at least a distance M from the hyperplane. Hence, M represents the margin of our hyperplane, and the optimization problem chooses $\beta_0, \beta_1, \beta_2, \dots, \beta_p$ to maximize M. \\
Unfortunately this hyperplane does not necessarily exists, but we can extend this concept to find a hyperplane that almost separates the classes using a soft margin. This is what an SVM does.

It is very probable that the data is not linearly separable, as we can see in figure \ref{fig:svmnotsep}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{svm_not_separable}
	\caption{Linearly not separable data \cite{svm_monkeylearn}}
	\label{fig:svmnotsep}
\end{figure}

In this case, for example, we can add a new dimension and separate the data. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{svm_three_dim}
	\caption{Three dimensional separable space \cite{svm_monkeylearn}}
\end{figure} 

And then map back to two dimension.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{svm_separated}
	\caption{Separated data \cite{svm_monkeylearn}}
\end{figure}



Calculating the transformation can be very computationally intensive, but SVM just needs the dot product between the vectors. This is called the kernel function. Kernels can be of different types, like linear or 
