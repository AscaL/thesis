%TODO: obviously rewrite this
In this section I will explain what I've done and the stack used for this thesis: I will start by reviewing the OpenFace tool (\ref{OpenFace}), the database (\ref{rldb}) utilized and all the experiments and techniques used, such as.....

%TODO: pipeline OPENFACE + analysis. here or architecture?
%TODO: SHORT review (table pherhaps) of face db, maybe not in this section

\section{OpenFace} \label{OpenFace}
The OpenFace \cite{Baltru2018} toolkit is a tool for machine learning and computer vision researchers created ba Baltrusaitis et al. to perform facial landmark detection, head pose estimation, action unit recognition and eye gaze estimation. \\

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{openface20_pipeline}
	\caption{OpenFace Pipeline \cite{Baltru2018}}
	\label{fig:openface20_pipeline}
\end{figure}

\subsection{Landmark Detection}
The first step in Action Unit identification is to detect the facial landmarks. To accomplish this a local detector called Convolutional Experts Network (CEN) \ref{fig:CEN} is utilized. CEN has the advantage of aggregating a neural architecture and patch experts (local detectors, they evaluate the probability of a landmark being aligned at a particular pixel location). CEN can learn different patch experts and adapt to diverse appearance models without explicit attribute labeling. \\

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{CEN}
	\caption{Facial landmarks naturally cluster around appearance prototypes (facial hair, expressions, make-up etc). To model such appearance variations a Convolutional Experts Network (CEN) is used to bring together the advantages of neural architectures and mixtures of patch experts to model landmark alignment probability. \cite{Baltru2017}.}
	\label{fig:CEN}
\end{figure}

%TODO: write better?
OpenFace uses a Convolutional Experts Constrained Local Model (CE-CLM) \cite{Baltru2017}, which is a Constrained Local Model (CLM) that uses CEN as a local detector. CLM are used to model the appearance of each facial landmark individually by using local detectors and a shape model for constrained optimization.\\
The CE-CLM is divided in two fundamental parts: response map computation using CEN, and shape parameter update. In the first step, the landmarks alignment is computed separately from the other landmarks. In the second phase, all landmarks are considered together and for misaligned landmarks and irregular shapes their position is penalized, using a Point Distribution Model (PDM).\\

\subsubsection{CEN}
In the first step the objective is to generate a response map to localize the individual landmarks. This is achieved by assessing the landmark alignment probability at specific pixel locations. CEN takes in input a Region of Interest (ROI) around the currently estimated position of a landmark, and outputs a response map that calculates the landmark alignment probability at each pixel location (Fig. \ref{fig:landmark_det}). \\
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{landmark_det}
	\caption{Overview of the Convolutional Experts Network model. The output response map is a non-negative and non-linear combination of neurons in ME-layer using a sigmoid activation \cite{Baltru2017}.}
	\label{fig:landmark_det}
\end{figure}
%TODO: to specific?
In order to do all this, the ROI is initially passed in a Contrast Normalizing Convolution layer to perform z-score normalization and to calculate the correlation between input and kernel. The output is then convolved in another layer of ReLU neurons (convolution is an operation on two functions to produce a third function that expresses how the shape of one is modified by the other). \\
The last neural layer before the response map is the Mixture of Expert Layer (ME-layer), and it can model the alignment probability using a combination of patch experts (local detectors) that are able to represent different landmarks appearance prototypes by outputting individual votes on alignment through a sigmoid function. The response maps from all the local detectors are then combined in the final layer, giving a final alignment probability.

\subsubsection{Point Distribution Model}
%TODO: from wiki modify a bit
The Point Distribution Model is a used to represent the mean geometry of a shape and some statistical modes of geometric variation inferred from a training set of shapes \cite{wiki:PDM}. Point distribution models rely on landmark points. The general PDM works this way: \\
First, a set of training images are manually landmarked to sufficiently approximate the geometry of the original shapes.\\
Then the shape outlines are reduced to sequences of k landmarks, so that a given training shape is defined as the vector $\mathbf {X} \in {R} ^{2k}$.\\
The matrix of the top $d$ eigenvectors is given as $\mathbf {P} \in  {R}^{2k\times d}$, and each eigenvector describes a principal mode of variation along the set.\\
Finally, a linear combination of the eigenvectors is used to define a new shape $ \mathbf {X} '$, mathematically defined as: \\ 
$\mathbf {X}' = {\overline {\mathbf {X}} + \mathbf{P} \mathbf{b}}$ \\
where $ {\overline {\mathbf {X}}}$ is defined as the mean shape across all training images, and $\mathbf {b}$  is a vector of scaling values for each principal component. Therefore, by modifying the variable $\mathbf {b}$  an infinite number of shapes can be defined. \\

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{PDM}
	\caption{PDM of a metacarpal. Dots mark the possible position of landmarks, and the line denote the mean shape \cite{PDM}.}
	\label{fig:PDM}
\end{figure}

In the OpenFace framework Point Distribution Model \cite{PDM_RLMS} models the location of facial feature points in the image using non-rigid shape and rigid global transformation parameters, and is utilized in the second phase of the algorithm.\\
The application of PDM has two objectives: first, they are employed to control the landmark locations, second they are used to regularize the shapes in the CE-CLM framework by using	Non-Uniform Regularized Landmark Mean Shift (NU-RLMS) \cite{Baltru2013}. In fact, in the final detected landmarks, the irregular shapes are imposed a penalty. \\

\section{Action Unit Detection}
%TODO: rewrite
Action Unit (AU) detection plays a fundamental role in our work. We now describe the process used to detect the AUs.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{AU_pipeline}
	\caption{AU detection and intensity pipeline \cite{Baltru2015}.}
	\label{fig:AU_pipeline}
\end{figure}

\subsection{Dataset}
There are three main dataset used for training the Action Unit detection system:  DISFA \cite{DISFA}, BP4D-Spontaneous \cite{BP4D-Spontaneous} and SEMAINE \cite{SEMAINE}. These three datasets consist of video of people subject to emotion inducing tasks.\\

The BP4D database of spontaneous facial expressions includes videos of 41 participants (23 women and 18 men, 21 for training and 20 for validating). The age ranged from 18 to 29; 11 were Asian, 6 African-American, 4 Hispanic, and 20 Euro-American. Emotion inductions techniques were used to elicit expressions of emotion. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Each participant is associated with 8 tasks. For each task, there are both 3D and 2D videos and the meta-data includes annotations for 11 AUs for occurrence and 5 AUs for intensities.\\

The annotated SEMAINE subset contains recordings of 31 subjects (15 for training and 16 for validation). It consists of one minute long recordings, leading to 93k frames labeled for 5 AU occurrences.\\

DISFA (Denver Intensity of Spontaneuos Facial Action) Database is a non posed facial expression database for automatic action unit detection. It contains videos of 27 participants (12 female, 15 males; 14 used for training and 13 for validation). It includes 4 minute-long videos of spontaneous facial expression, resulting in 130k frames annotated for 12 AUs (Fig. \ref{fig:DISFA_AU}), comprehensive of AUs intensity on a 0 to 5 scale.\\

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{DISFA_AU}
	\caption{AUs coded in the DISFA database \cite{DISFA_AU}.}
	\label{fig:DISFA_AU}
\end{figure}
All of the datasets have three AUs in common (2, 12, and 17). SEMAINE and DISFA share AUs 2, 12, 17, 25. BP4D and DISFA share AUs 1, 2, 4, 6, 12, 15, 17. This allows for cross-dataset training.\\

%todo: complete dataset description 
Other dataset included in the study are:
CK+ \cite{CK+}
FERA 2011 \cite{FERA11}
FERA 2015 \cite{FERA15}
AVEC 2011 \cite{AVEC11}



\subsection{Feature Extraction}
There are two types of features that are used: appearance and geometry ones. To extract those features it's required to track certain landmarks on a face, followed by face alignment.\\

\subsubsection{Face Tracking}
%TODO: rewrite
Constrained Local Neural Field (CLNF) [2] facial landmark detector and tracker for face tracking and to extract geometry based features (explained in Section IV-D). We use the open source CLNF implementation [2]. It uses a Structural SVM face detector [13], followed by CLNF.
CLNF is an instance of a Constrained Local Model (CLM) [5], that uses more advanced patch experts and optimisation function. The model we used was trained on Multi-PIE [11] and in-the-wild [21] facial datasets.
%The CLM model we use can be described by parameters p = [s, R, p, t] that can be varied to acquire various instances of the model: the scale factor s; object rotation R (ﬁrst two rows of a 3D rotation matrix); 2D translation t; a vector describing non-rigid variation of shape p. The point distribution model (PDM) is:

%x i = s · R(x i + Φ i p) + t.

%Here x i = (x, y) denotes the 2D location of the i th feature point in an image, x i = (X, Y, Z) is the mean value of the i th element of the PDM in the 3D reference frame, and the vector Φ i is the i th eigenvector obtained from the training set that describes the linear variations of non-rigid shape of this feature point, and the vector Ψ i is the i th eigenvector that describes the linear variations of non-rigid shape.

In CLM (and CLNF), we estimate the maximum a posteriori probability (MAP) of the face model parameters p given an initial location of the parameters determined by a face detection step.

\subsubsection{Alignment and Masking}
For the extracted face to be correctly analyzed, there needs to be a mapping to a common reference frame, and the changes resulting from scaling and in plane rotation need to be removed. \\
In order to achieve this, a similarity transform from the currently detected landmarks to a representation of frontal landmarks from a neutral expression (projection of mean shape from a 3D PDM) is utilized. The similarity transform is done with Procrustes superimposition that minimizes the mean square error between aligned pixels.\\
The result is a $112 \times 112$ pixel image of the face with 45 pixel interpupillary distance. \\
%TODO: tocca capire sta cosa del CLNF ecc
To reduce the weight of significant facial expressions (mouth opening, brow raises etc.) on the similarity transform, only the most stable facial landmarks are used. \\
%TODO: non mi piace
To determine those landmarks, the most stable CLNF detected landmark points under facial expression on the on CK+ dataset \cite{CK+} are taken (Fig. \ref{fig:alignment_masking}). CK+ is a dataset containing videos of people performing diverse facial expressions, mostly starting from a neutral pose, while the head is still.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{alignment_masking}
	\caption{Stable points used for alignment to a common reference frame, followed by masking. \cite{Baltru2015}.}
	\label{fig:alignment_masking}
\end{figure}

Masking, performed using a convex hull surrounding the feature points, is done to remove non face relevant information from the images.

\subsubsection{Appearance Features}
After the face is aligned to the $112 \times 112$ image it's time to extract the appearance features. To get the appearance features, $12 \times 12$ block of 31 dimensional Histogram of Oriented Gradients (HOG) are extracted, giving a 4464 dimensional vector characterizing the face. The implementation of HoG utilized comes from dlib \cite{dlib}.\\

Once the feature vector is obtained, the next step is to reduce it's dimensionality. In order to do that Principal Component Analysis (PCA) is applied. To generalize the dimensionality reduction, the training was performed on the FERA 2015 \cite{FERA15}, CK+ \cite{CK+}, DISFA \cite{DISFA}, AVEC 2011 \cite{AVEC11} and FERA 2011 \cite{FERA11} datasets. By using PCA while sub-sampling from peak and neutral expressions and keeping 95\% of explained variability, the feature vector becomes of 1379 dimensions.

\subsubsection{Geometry Features}
%TODO: review all this
For geometry features, we used the non-rigid shape parameters and landmark locations in object space inferred during CLNF model tracking. This led to a 23 + 204 = 227 dimensional vector describing geometry.\\
Together both of the descriptors led to a 1606 dimensional vector, that describes appearance of the face. This feature vector is used in all of the following experiments.















\section{Real Life Trial DataBase} \label{rldb}

\section{GLM}

\section{LDA}

\section{QDA}

\section{SVM}

\section{Correlations} ?