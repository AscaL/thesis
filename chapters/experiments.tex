%TODO: obviously rewrite this
In this section I will explain what I've done and the stack used for this thesis: I will start by reviewing the OpenFace tool (\ref{OpenFace}), the database (\ref{rldb}) utilized and all the experiments and techniques used, such as.....

%TODO: SHORT review (table pherhaps) of face db, maybe not in this section


\section{OpenFace} \label{OpenFace}
The OpenFace \cite{Baltru2018} toolkit is a tool for machine learning and computer vision researchers created ba Baltrusaitis et al. to perform facial landmark detection, head pose estimation, action unit recognition and eye gaze estimation.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{openface20_pipeline}
	\caption{OpenFace Pipeline \cite{Baltru2018}}
	\label{fig:openface20_pipeline}
\end{figure}

\subsection{Landmark Detection}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{land_det_ex}
	\caption{Example of different facial landmarks detected with OpenFace in different conditions and viewing angles. \cite{Baltru2018}.}
	\label{fig:land_det_ex}
\end{figure}

The first step in Action Unit identification is to detect the facial landmarks. To accomplish this a local detector called Convolutional Experts Network (CEN) \ref{fig:CEN} is utilized. \\
CEN has the advantage of aggregating a neural architecture and patch experts (local detectors, they evaluate the probability of a landmark being aligned at a particular pixel location). CEN can learn different patch experts and adapt to diverse appearance models without explicit attribute labeling.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{CEN}
	\caption{Facial landmarks naturally cluster around appearance prototypes (facial hair, expressions, make-up etc). To model such appearance variations a Convolutional Experts Network (CEN) is used to bring together the advantages of neural architectures and mixtures of patch experts to model landmark alignment probability \cite{Baltru2017}.}
	\label{fig:CEN}
\end{figure}

%TODO: write better?
OpenFace uses a Convolutional Experts Constrained Local Model (CE-CLM) \cite{Baltru2017}, which is a Constrained Local Model (CLM) that uses CEN as a local detector. 

A Constrained Local Model is class of methods for locating sets of points (constrained by a statistical shape model) on a target image \cite{clm_cootes}.
Generally the procedure is as follows:
\begin{itemize}
	\item Sample a region from the image around the current estimate, projecting it into a reference frame.
	\item For each point, generate a "response image" giving a cost for having the point at each pixel.
	\item Search for a combination of points which optimizes the total cost, by manipulating the shape model parameters.
\end{itemize}

%todo: pic from https://pdfs.semanticscholar.org/c2a3/850becae2799b0e591e7ab1008b84897d6e9.pdf
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{CLM}
	\caption{Overview of CLM \cite{clm_cootes}.}
	\label{fig:CLM}
\end{figure}
 
CLM are used to model the appearance of each facial landmark individually by using local detectors and a shape model for constrained optimization. 

The CE-CLM is divided in two fundamental parts: response map computation using CEN, and shape parameter update. \\
In the first step, the landmarks alignment is computed separately from the other landmarks. \\
In the second phase, all landmarks are considered together and for misaligned landmarks and irregular shapes their position is penalized, using a Point Distribution Model (PDM).

\subsubsection{CEN}
In the first step, the objective is to generate a response map to localize the individual landmarks. This is achieved by assessing the landmark alignment probability at specific pixel locations. \\
CEN takes in input a Region of Interest (ROI) around the currently estimated position of a landmark, and outputs a response map that calculates the landmark alignment probability at each pixel location (Fig. \ref{fig:landmark_det}).

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{landmark_det}
	\caption{Overview of the Convolutional Experts Network model. The output response map is a non-negative and non-linear combination of neurons in ME-layer using a sigmoid activation \cite{Baltru2017}.}
	\label{fig:landmark_det}
\end{figure}

%TODO: to specific?
In order to do all this, the ROI is initially passed in a Contrast Normalizing Convolution layer to perform z-score normalization and to calculate the correlation between input and kernel. The output is then convolved in another layer of ReLU neurons (convolution is an operation on two functions to produce a third function that expresses how the shape of one is modified by the other).

The last neural layer before the response map is the Mixture of Expert Layer (ME-layer), and it can model the alignment probability using a combination of patch experts (local detectors) that are able to represent different landmarks appearance prototypes by outputting individual votes on alignment through a sigmoid function. The response maps from all the local detectors are then combined in the last layer, giving the final alignment probability.

\subsubsection{Point Distribution Model}
%TODO: from wiki modify a bit
The Point Distribution Model is a used to represent the mean geometry of a shape and some statistical modes of geometric variation inferred from a training set of shapes \cite{wiki:PDM}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{PDM}
	\caption{PDM of a metacarpal. Dots mark the possible position of landmarks, and the line denote the mean shape \cite{PDM}.}
	\label{fig:PDM}
\end{figure}

Point distribution models rely on landmark points. The general PDM works this way:
\begin{enumerate}
	\item A set of training images are manually landmarked to sufficiently approximate the geometry of the original shapes. 
	\item This \textit{k} landmarks are aligned in two dimensions resulting in \\
	$\mathbf{X} = (x_1,y_1, \dots, x_k, y_k)$
	\item The shape outlines are reduced to sequences of k landmarks, so that any given training shape can be defined by the vector $\mathbf{X} \in {\mathbb{R} ^{2k}}$.
	\item The matrix of the top $d$ eigenvectors is given as $\mathbf{P} \in \mathbb{R}^{2k \times d}$, and each eigenvector describes a principal mode of variation along the set.
	\item A linear combination of the eigenvectors is used to define a new shape $ \mathbf{X} '$, mathematically defined as: 
		\begin{equation}
			\mathbf {X}' = {\overline {\mathbf {X}} + \mathbf{P} \mathbf{b}}
		\end{equation}
		
	where $ {\overline {\mathbf {X}}}$ is defined as the mean shape across all training images, and $\mathbf {b}$ is a vector of scaling values for each principal component. 
	\item By modifying the variable $\mathbf {b}$  an infinite number of shapes can be defined. $\mathbf {b}$ shouldn't generally be modified more than $\pm3\sigma$ \cite{wiki:PDM}.
\end{enumerate}

In the OpenFace framework Point Distribution Model \cite{PDM_RLMS} models the location of facial feature points in the image using non-rigid shape and rigid global transformation parameters, and is utilized in the second phase of the algorithm.

The application of PDM has two objectives:

\begin{itemize}
	\item They are employed to control the landmark locations.
	\item They are used to regularize the shapes in the CE-CLM framework by using Non-Uniform Regularized Landmark Mean Shift (NU-RLMS) \cite{Baltru2013}.
\end{itemize}
This has an effect in the final detected landmarks, where the irregular shapes are imposed a penalty.

\subsection{Action Unit Detection}
%TODO: rewrite
Action Unit (AU) detection plays a fundamental role in our work. We now describe the process used to detect the AUs, starting with an overview of the utilized databases for AU detection.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.1\textwidth]{AU_pipeline}
	\caption{AU detection and intensity pipeline \cite{Baltru2015}.}
	\label{fig:AU_pipeline}
\end{figure}

\subsubsection{Datasets}
There are three main dataset used for training the Action Unit detection system: DISFA \cite{DISFA}, BP4D-Spontaneous \cite{BP4D-Spontaneous} and SEMAINE \cite{SEMAINE}. These three datasets consist of video of people subject to emotion inducing tasks.

The BP4D database of spontaneous facial expressions includes videos of 41 participants (23 women and 18 men, 21 for training and 20 for validating). The age ranged from 18 to 29; 11 were Asian, 6 African-American, 4 Hispanic, and 20 Euro-American. \\
Emotion inducing techniques were used to elicit an emotional response. Frame-level ground-truth for facial actions was obtained by using the Facial Action Coding System annotations, performed by trained professionals. \\
Each participant in the database is associated with 8 tasks. For each task, there are both 3D and 2D videos and the metadata include annotations for 11 AUs for occurrence and 5 AUs for intensities.

The FACS annotated SEMAINE subset contains recordings of 31 subjects (15 for training and 16 for validation). It consists of one minute long recordings, leading to 93k frames labeled for 5 AU occurrences.

DISFA (Denver Intensity of Spontaneuos Facial Action) Database is a non posed facial expression database for automatic action unit detection. It contains videos of 27 participants (12 female, 15 males; 14 used for training and 13 for validation). It includes 4 minute-long videos of spontaneous facial expression, resulting in 130k frames annotated for 12 AUs (Fig. \ref{fig:DISFA_AU}), comprehensive of AUs intensity on a 0 to 5 scale.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{DISFA_AU}
	\caption{AUs coded in the DISFA database \cite{DISFA_AU}.}
	\label{fig:DISFA_AU}
\end{figure}

All of the datasets have three AUs in common (2, 12, and 17). \\
SEMAINE and DISFA share AUs 2, 12, 17, 25. \\
BP4D and DISFA share AUs 1, 2, 4, 6, 12, 15, 17. This allows for cross-dataset training.

%todo: complete dataset description 
Other dataset included in the study are:
\begin{itemize}[noitemsep]
	\item CK+ \cite{CK+}: 
	\item FERA 2011 \cite{FERA11}:
	\item FERA 2015 \cite{FERA15}:
	\item AVEC 2011 \cite{AVEC11}:
\end{itemize}


\subsubsection{Feature Extraction}
There are two types of features that are used: appearance and geometry ones. To extract those features it's required to track certain landmarks on the face, and then continue this process by performing face alignment.

\paragraph{Face Tracking}

%TODO: rewrite
Face tacking is done by utilizing Constrained Local Neural Field (CLNF) facial landmark detector and tracker, backed up by a structural SVM for facial detection \cite{Baltru2013}.
CLNF is a specific case of Constrained Local Model (CLM), that differs from the original by utilizing more advanced local detectors and a different optimization function.

The Constrained Local Model can be described by the following parameters: \\
$p = [s, \mathbf{R}, \mathbf{p}, \mathbf{t}]$.
\begin{itemize}[noitemsep, topsep = -5pt]
	\item \textit{s} is the scale factor.
	\item \textbf{R} is the object rotation.
	\item \textbf{t} is the 2D translation.
	\item \textbf{p} describes the shape of a vector of non rigid variations.
\end{itemize}

These parameters can be modified to compute different versions of the model. The resulting point distribution model is:

\begin{equation} \label{eq:pdm}
	x_i = s \cdot \mathbf{R}(\overline{x_i} + \phi_i \mathbf{p}) + \mathbf{t}
\end{equation}

Where $x_i$ is the location of the i\textit{th} feature point in an image, $\overline{x_i}$ is the mean of the i\textit{th} element of the PDM, and $\phi_i$ is the i\textit{th} eigenvector that describes the variation of the feature point.

In the Constrained Local Model we use the parameters from the face detection to estimate the maximum a posteriori probability \textit{p} of the face model.

\paragraph{Alignment and Masking}
For the extracted face to be correctly analyzed, there needs to be a mapping to a common reference frame, and the changes resulting from scaling and in plane rotation need to be removed. 

In order to achieve this, a similarity transform from the currently detected landmarks to a representation of frontal landmarks from a neutral expression (projection of mean shape from a 3D PDM) is utilized. The similarity transform is done with Procrustes superimposition that minimizes the mean square error between aligned pixels \cite{Baltru2013}.\\
The result is a $112 \times 112$ pixel image of the face with 45 pixel interpupillary distance (Fig \ref{fig:alignment_masking}). 

%TODO: tocca capire sta cosa del CLNF ecc
To reduce the weight of significant facial expressions (mouth opening, brow raises etc.) on the similarity transform, only the most stable facial landmarks must be used. \\
%TODO: non mi piace
In order to determine those points, the most stable CLNF detected landmarks on the CK+ dataset \cite{CK+} are examined. CK+ is a dataset containing videos of people performing diverse facial expressions, mostly starting from a neutral pose, while the head is still.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{alignment_masking}
	\caption{Stable points used for alignment to a common reference frame, followed by masking. \cite{Baltru2015}.}
	\label{fig:alignment_masking}
\end{figure}

Masking, performed using a convex hull surrounding the feature points, is done to remove non face relevant information from the images.

\paragraph{Appearance Features}
After the face is aligned to the $112 \times 112$ image it's time to extract the appearance features. \\
To get the appearance features, $12 \times 12$ block of 31 dimensional Histogram of Oriented Gradients (HOG) are extracted, giving a 4464 dimensional vector characterizing the face. The implementation that HoG utilized comes from dlib \cite{dlib}.

Once the feature vector is obtained, the next step is to reduce it's dimensionality. In order to do that Principal Component Analysis (PCA) is applied. \\
To generalize the dimensionality reduction, the training was performed on the FERA 2015 \cite{FERA15}, CK+ \cite{CK+}, DISFA \cite{DISFA}, AVEC 2011 \cite{AVEC11} and FERA 2011 \cite{FERA11} datasets. By using PCA while sub-sampling from peak and neutral expressions and keeping 95\% of explained variability, the feature vector becomes of 1379 dimensions.

\paragraph{Geometry Features}
%TODO: CHECK CLNF STUFF
The geometry features are obtained trough the CE-CLM models, and consist of non-rigid shape parameters and landmarks locations ($p$ and $ \phi_i p$ in equation \ref{eq:pdm}). this results in a 227 dimensional vector representing geometry features.\\
Summed to the appearance features, this leads to a 1606 dimensional vector that defines the appearance of the face.

\paragraph{Neutral Expression Extraction}
To extract some of the facial expression is very important to have a neutral starting expression. There are personal differences on how we appear while in a neutral resting state, such as people looking naturally more cheerful or sad then others \cite{normexpr}. To address this issue there needs to be a person specific calibration, done by adjusting for neutral expressions \cite{Baltru2013}.

Neutral expression adjustments are done by computing the median value of the face descriptors in a video, leading to a neutral expression descriptor. This works assuming that in a video most of the frames will contain a neutral expression, and this should hold true especially for real life situations where most of the time the interactions are performed with a neutral expression \cite{NatAffData}.

Once the neutral descriptor is computer, it is subtracted from the feature descriptor, giving normalized features. To help with the ease of computing the median, a histogram is kept for each element in the feature vector.

\paragraph{Classification and Regression}
Action Units detection is performed through Support Vector Machines (SVM), and Action Unit intensity using Support Vector Regression (SVR). In both cases a linear kernel is utilized, as more complex kernels had no effect on performances and were quite slower. 

Since the AU occurrences are not balanced by nature, it's highly important to balance the training data. This was done by under-sampling the negative AU samples from training data, leading to an equal number of positive and negative samples.

\section{Real Life Trial DataBase} \label{rldb}
This section is about the database we used to perform our experiments: it comes from the work done for the paper "Deception Detection using Real-life Trial Data" \cite{Perez-Rosas:2015:DDU:2818346.2820758}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{trial_images}
	\caption{Examples of images from the dataset videos. \cite{Perez-Rosas:2015:DDU:2818346.2820758}.}
	\label{fig:trial_images}
\end{figure}


The dataset is gathered from real-life trial videos available on YouTube and other public websites. The dataset also contains statements made by exonerees after exoneration, and some statements from defendants during crime-related TV episodes.

The first step to collecting the dataset was to identify public multimedia sources where the recordings of the trials were available, and deceptive and truthful behavior could be observed and verified.\\
The videos are of trial recordings where the defendant or witness in the video can be clearly identified, the face is visible enough during most of the clip duration, and the visual quality should be good enough to accurately see the facial expressions (Fig. \ref{fig:trial_images}).\\
There are three outcomes for the trials that were considered to label the videos as deceptive or truthful: guilty, non-guilty, and exoneration. \\
For the guilty verdicts the deceptive clips are taken from the defendant in the trial, while the truthful clips are gathered from the witnesses. There are also instances where the deceptive videos are of suspects denying a committed crime, and truthful ones are from the same person answering questions that where verified by the police as truthful.

In regards to the witnesses, if the testimony is verified by a police officer they are labeled as true. \\ Testimonies that help the guilty party are labeled as false. Exoneration (reversal of the sentence) testimonies are regarded as truthful.

The original dataset consists of 121 videos, 61 of which are deceptive and 60 truthful. \\
The average length of the videos is 28.0 seconds. The average video length for deceptive videos is 27.7 seconds, while the one for truthful videos is 28.3 seconds. \\
The data consists of 56 total subject, 21 unique females and 35 unique males, with ages between 16 and 60 years.

%TODO: modification made by me to the DB 
%TODO: rewrite tentative atm

We modified this database to reduce the amount of videos where the face was hard to see, that had multiple subjects, or where the subjects were not visible while speaking.
Also we performed a division of subjects to train the classifiers in a way that it wouldn't "remember" the person, since we have many videos with the same subject. 
the same person never appears in the 


\section{GLM}

\section{LDA}

\section{QDA}

\section{SVM}

\section{Correlations} ?