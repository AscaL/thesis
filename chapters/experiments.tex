%TODO: obviously rewrite this
In this section I will explain what I've done and the stack used for this thesis: I will start by reviewing the OpenFace tool (\ref{OpenFace}), the database (\ref{rldb}) utilized and all the experiments and techniques used, such as.....

%TODO: pipeline OPENFACE + analysis. here or architecture?
%TODO: SHORT review (table pherhaps) of face db, maybe not in this section


\section{OpenFace} \label{OpenFace}
The OpenFace \cite{Baltru2018} toolkit is a tool for machine learning and computer vision researchers created ba Baltrusaitis et al. to perform facial landmark detection, head pose estimation, action unit recognition and eye gaze estimation.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{openface20_pipeline}
	\caption{OpenFace Pipeline \cite{Baltru2018}}
	\label{fig:openface20_pipeline}
\end{figure}

\subsection{Landmark Detection}
The first step in Action Unit identification is to detect the facial landmarks. To accomplish this a local detector called Convolutional Experts Network (CEN) \ref{fig:CEN} is utilized. \\CEN has the advantage of aggregating a neural architecture and patch experts (local detectors, they evaluate the probability of a landmark being aligned at a particular pixel location). CEN can learn different patch experts and adapt to diverse appearance models without explicit attribute labeling.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{CEN}
	\caption{Facial landmarks naturally cluster around appearance prototypes (facial hair, expressions, make-up etc). To model such appearance variations a Convolutional Experts Network (CEN) is used to bring together the advantages of neural architectures and mixtures of patch experts to model landmark alignment probability \cite{Baltru2017}.}
	\label{fig:CEN}
\end{figure}

%TODO: write better?
OpenFace uses a Convolutional Experts Constrained Local Model (CE-CLM) \cite{Baltru2017}, which is a Constrained Local Model (CLM) that uses CEN as a local detector. CLM are used to model the appearance of each facial landmark individually by using local detectors and a shape model for constrained optimization. \\
The CE-CLM is divided in two fundamental parts: response map computation using CEN, and shape parameter update. In the first step, the landmarks alignment is computed separately from the other landmarks. In the second phase, all landmarks are considered together and for misaligned landmarks and irregular shapes their position is penalized, using a Point Distribution Model (PDM).

%TODO: good position for this?
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{land_det_ex}
	\caption{Example of landmarks detected with the OpenFace implementation \cite{Baltru2018}.}
	\label{fig:land_det_ex}
\end{figure}

\subsubsection{CEN}
In the first step the objective is to generate a response map to localize the individual landmarks. This is achieved by assessing the landmark alignment probability at specific pixel locations. CEN takes in input a Region of Interest (ROI) around the currently estimated position of a landmark, and outputs a response map that calculates the landmark alignment probability at each pixel location (Fig. \ref{fig:landmark_det}).

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{landmark_det}
	\caption{Overview of the Convolutional Experts Network model. The output response map is a non-negative and non-linear combination of neurons in ME-layer using a sigmoid activation \cite{Baltru2017}.}
	\label{fig:landmark_det}
\end{figure}

%TODO: to specific?
In order to do all this, the ROI is initially passed in a Contrast Normalizing Convolution layer to perform z-score normalization and to calculate the correlation between input and kernel. The output is then convolved in another layer of ReLU neurons (convolution is an operation on two functions to produce a third function that expresses how the shape of one is modified by the other).

The last neural layer before the response map is the Mixture of Expert Layer (ME-layer), and it can model the alignment probability using a combination of patch experts (local detectors) that are able to represent different landmarks appearance prototypes by outputting individual votes on alignment through a sigmoid function. The response maps from all the local detectors are then combined in the final layer, giving a final alignment probability.

\subsubsection{Point Distribution Model}
%TODO: from wiki modify a bit
The Point Distribution Model is a used to represent the mean geometry of a shape and some statistical modes of geometric variation inferred from a training set of shapes \cite{wiki:PDM}. 

Point distribution models rely on landmark points. The general PDM works this way:

First, a set of training images are manually landmarked to sufficiently approximate the geometry of the original shapes.\\
Then the shape outlines are reduced to sequences of k landmarks, so that a given training shape is defined as the vector $\mathbf{X} \in {\mathbb{R} ^{2k}}$.\\
The matrix of the top $d$ eigenvectors is given as $\mathbf{P} \in \mathbb{R}^{2k \times d}$, and each eigenvector describes a principal mode of variation along the set.\\
Finally, a linear combination of the eigenvectors is used to define a new shape $ \mathbf{X} '$, mathematically defined as: \\ 
$\mathbf {X}' = {\overline {\mathbf {X}} + \mathbf{P} \mathbf{b}}$ \\
where $ {\overline {\mathbf {X}}}$ is defined as the mean shape across all training images, and $\mathbf {b}$ is a vector of scaling values for each principal component. Therefore, by modifying the variable $\mathbf {b}$  an infinite number of shapes can be defined.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{PDM}
	\caption{PDM of a metacarpal. Dots mark the possible position of landmarks, and the line denote the mean shape \cite{PDM}.}
	\label{fig:PDM}
\end{figure}

In the OpenFace framework Point Distribution Model \cite{PDM_RLMS} models the location of facial feature points in the image using non-rigid shape and rigid global transformation parameters, and is utilized in the second phase of the algorithm.\\
The application of PDM has two objectives: first, they are employed to control the landmark locations, second they are used to regularize the shapes in the CE-CLM framework by using	Non-Uniform Regularized Landmark Mean Shift (NU-RLMS) \cite{Baltru2013}. In fact, in the final detected landmarks, the irregular shapes are imposed a penalty.

\subsection{Action Unit Detection}
%TODO: rewrite
Action Unit (AU) detection plays a fundamental role in our work. We now describe the process used to detect the AUs.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{AU_pipeline}
	\caption{AU detection and intensity pipeline \cite{Baltru2015}.}
	\label{fig:AU_pipeline}
\end{figure}

\subsubsection{Dataset}
There are three main dataset used for training the Action Unit detection system:  DISFA \cite{DISFA}, BP4D-Spontaneous \cite{BP4D-Spontaneous} and SEMAINE \cite{SEMAINE}. These three datasets consist of video of people subject to emotion inducing tasks.

The BP4D database of spontaneous facial expressions includes videos of 41 participants (23 women and 18 men, 21 for training and 20 for validating). The age ranged from 18 to 29; 11 were Asian, 6 African-American, 4 Hispanic, and 20 Euro-American. Emotion inductions techniques were used to elicit expressions of emotion. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Each participant is associated with 8 tasks. For each task, there are both 3D and 2D videos and the meta-data includes annotations for 11 AUs for occurrence and 5 AUs for intensities.

The annotated SEMAINE subset contains recordings of 31 subjects (15 for training and 16 for validation). It consists of one minute long recordings, leading to 93k frames labeled for 5 AU occurrences.

DISFA (Denver Intensity of Spontaneuos Facial Action) Database is a non posed facial expression database for automatic action unit detection. It contains videos of 27 participants (12 female, 15 males; 14 used for training and 13 for validation). It includes 4 minute-long videos of spontaneous facial expression, resulting in 130k frames annotated for 12 AUs (Fig. \ref{fig:DISFA_AU}), comprehensive of AUs intensity on a 0 to 5 scale.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{DISFA_AU}
	\caption{AUs coded in the DISFA database \cite{DISFA_AU}.}
	\label{fig:DISFA_AU}
\end{figure}

All of the datasets have three AUs in common (2, 12, and 17). SEMAINE and DISFA share AUs 2, 12, 17, 25. BP4D and DISFA share AUs 1, 2, 4, 6, 12, 15, 17. This allows for cross-dataset training.

%todo: complete dataset description 
Other dataset included in the study are:
\begin{itemize}[noitemsep]
	\item CK+ \cite{CK+}:
	\item FERA 2011 \cite{FERA11}:
	\item FERA 2015 \cite{FERA15}:
	\item AVEC 2011 \cite{AVEC11}:
\end{itemize}



\subsubsection{Feature Extraction}
There are two types of features that are used: appearance and geometry ones. To extract those features it's required to track certain landmarks on a face, followed by face alignment.

\paragraph{Face Tracking}
%TODO: rewrite
Constrained Local Neural Field (CLNF) [2] facial landmark detector and tracker for face tracking and to extract geometry based features (explained in Section IV-D). We use the open source CLNF implementation [2]. It uses a Structural SVM face detector [13], followed by CLNF.
CLNF is an instance of a Constrained Local Model (CLM) [5], that uses more advanced patch experts and optimisation function. The model we used was trained on Multi-PIE [11] and in-the-wild [21] facial datasets.
%The CLM model we use can be described by parameters p = [s, R, p, t] that can be varied to acquire various instances of the model: the scale factor s; object rotation R (Ô¨Årst two rows of a 3D rotation matrix); 2D translation t; a vector describing non-rigid variation of shape p. The point distribution model (PDM) is:

%x i = s ¬∑ R(x i + Œ¶ i p) + t.

%Here x i = (x, y) denotes the 2D location of the i th feature point in an image, x i = (X, Y, Z) is the mean value of the i th element of the PDM in the 3D reference frame, and the vector Œ¶ i is the i th eigenvector obtained from the training set that describes the linear variations of non-rigid shape of this feature point, and the vector Œ® i is the i th eigenvector that describes the linear variations of non-rigid shape.

In CLM (and CLNF), we estimate the maximum a posteriori probability (MAP) of the face model parameters p given an initial location of the parameters determined by a face detection step.

\paragraph{Alignment and Masking}
For the extracted face to be correctly analyzed, there needs to be a mapping to a common reference frame, and the changes resulting from scaling and in plane rotation need to be removed. \\
In order to achieve this, a similarity transform from the currently detected landmarks to a representation of frontal landmarks from a neutral expression (projection of mean shape from a 3D PDM) is utilized. The similarity transform is done with Procrustes superimposition that minimizes the mean square error between aligned pixels.\\
The result is a $112 \times 112$ pixel image of the face with 45 pixel interpupillary distance. \\
%TODO: tocca capire sta cosa del CLNF ecc
To reduce the weight of significant facial expressions (mouth opening, brow raises etc.) on the similarity transform, only the most stable facial landmarks are used. \\
%TODO: non mi piace
To determine those landmarks, the most stable CLNF detected landmark points under facial expression on the on CK+ dataset \cite{CK+} are taken (Fig. \ref{fig:alignment_masking}). CK+ is a dataset containing videos of people performing diverse facial expressions, mostly starting from a neutral pose, while the head is still.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{alignment_masking}
	\caption{Stable points used for alignment to a common reference frame, followed by masking. \cite{Baltru2015}.}
	\label{fig:alignment_masking}
\end{figure}

Masking, performed using a convex hull surrounding the feature points, is done to remove non face relevant information from the images.

\paragraph{Appearance Features}
After the face is aligned to the $112 \times 112$ image it's time to extract the appearance features. To get the appearance features, $12 \times 12$ block of 31 dimensional Histogram of Oriented Gradients (HOG) are extracted, giving a 4464 dimensional vector characterizing the face. The implementation of HoG utilized comes from dlib \cite{dlib}.

Once the feature vector is obtained, the next step is to reduce it's dimensionality. In order to do that Principal Component Analysis (PCA) is applied. To generalize the dimensionality reduction, the training was performed on the FERA 2015 \cite{FERA15}, CK+ \cite{CK+}, DISFA \cite{DISFA}, AVEC 2011 \cite{AVEC11} and FERA 2011 \cite{FERA11} datasets. By using PCA while sub-sampling from peak and neutral expressions and keeping 95\% of explained variability, the feature vector becomes of 1379 dimensions.

\paragraph{Geometry Features}
%TODO: CHECK CLNF STUFF
The geometry features are obtained trough the CE-CLM models, and consist of non-rigid shape parameters and landmarks locations. They resulted in a 227 dimensional vector representing geometry features.\\
Summed to the appearance features, this leads to a 1606 dimensional vector that defines the appearance of the face.

\paragraph{Neutral Expression Extraction}
To extract some of the facial expression is very important to have a neutral starting expression. There are personal differences on how we appear while in a neutral resting state, such as people looking more smily or frowny then others \cite{normexpr}. To address this issue there needs to be a person specific calibration, done by adjusting for neutral expressions.\\
This is done by computing the median value of the face descriptors in a video, leading to a neutral expression descriptor. This works assuming that in a video most of the frames will contain a neutral expression, this should hold true especially for real life situations where most of the time the interactions are performed with a neutral expression \cite{NatAffData}.\\
The median face descriptor is subtracted from the feature descriptor, giving normalized features. To help with the ease of computing the median, a histogram is kept for each element in the feature vector.

\paragraph{Classification and Regression}
Action Units detection is performed through Support Vector Machines (SVM), and Action Unit intensity using Support Vector Regression (SVR). In both cases a linear kernel is utilized, as more complex kernels had no effect on performances and were quite slower. \\
Since the AU occurrences are not balanced by nature, it's highly important to balance the training data. This was done by under-sampling the negative AU samples from training data, leading to an equal number of positive and negative samples.

\section{Real Life Trial DataBase} \label{rldb}
This section is about the database we used to perform our experiments: it comes from the work done for the paper "Deception Detection using Real-life Trial Data" \cite{Perez-Rosas:2015:DDU:2818346.2820758}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{trial_images}
	\caption{Examples of images from the dataset videos. \cite{Perez-Rosas:2015:DDU:2818346.2820758}.}
	\label{fig:trial_images}
\end{figure}


The dataset is gathered from real-life trial videos available on YouTube and other public websites. The dataset also contains statements made by exonerees after exoneration, and a few statements from defendants during crime-related TV episodes.

The first step to collecting the dataset was to identify public multimedia sources where the recordings of the trials were available, and deceptive and truthful behavior could be observed and verified.\\
The videos are of trial recordings where the defendant or witness in the video could be clearly identified, the face is visible enough during most of the clip duration, and the visual quality should be good enough to accurately see the facial expressions (Fig. \ref{fig:trial_images}).\\
There are three outcomes for the trials that were considered to label the videos as deceptive or truthful: guilty, non-guilty, and exoneration. \\
For the guilty verdicts the deceptive clips are taken from the defendant in the trial, while the truthful clips are gathered from the witnesses. There are also instances where the deceptive videos are of suspects denying a committed crime, and truthful ones are from the same person answering questions that where verified by the police as truthful.

In regards to the witnesses, when the testimony is verified by a police officer they are labeled as true. Testimonies that help the guilty party are labeled as false. Exoneration (reversal of the sentence) testimonies are regarded as truthful.

The dataset consists of 121 videos, 61 of which are deceptive and 60 truthful. The average length of the videos is 28.0 seconds. The average video length for deceptive videos is 27.7 seconds, while the one for truthful videos is 28.3 seconds. The data consists of 56 total subject, 21 unique females and 35 unique males, with ages between 16 and 60 years.

\section{GLM}

\section{LDA}

\section{QDA}

\section{SVM}

\section{Correlations} ?